{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea21cc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 분할/저장 요약 (파일별) ===\n",
      "results_ds_cloth_20250913_222703.json: eval_1(front)=238개 acc_excl_skip=60.08% | acc_incl_skip=60.08%  ||  eval_2(back)=238개 acc_excl_skip=65.55% | acc_incl_skip=65.55%\n",
      "results_ds_korean_20250913_222607.json: eval_1(front)=62개 acc_excl_skip=82.26% | acc_incl_skip=82.26%  ||  eval_2(back)=62개 acc_excl_skip=85.48% | acc_incl_skip=85.48%\n",
      "results_ds_race_high_long_20250913_222903.json: eval_1(front)=262개 acc_excl_skip=72.14% | acc_incl_skip=72.14%  ||  eval_2(back)=263개 acc_excl_skip=76.81% | acc_incl_skip=76.81%\n",
      "results_ds_race_high_short_20250913_223008.json: eval_1(front)=262개 acc_excl_skip=82.82% | acc_incl_skip=82.82%  ||  eval_2(back)=263개 acc_excl_skip=72.24% | acc_incl_skip=72.24%\n",
      "results_ds_race_middle_long_20250913_222726.json: eval_1(front)=87개 acc_excl_skip=77.01% | acc_incl_skip=77.01%  ||  eval_2(back)=88개 acc_excl_skip=84.09% | acc_incl_skip=84.09%\n",
      "results_ds_race_middle_short_20250913_222747.json: eval_1(front)=87개 acc_excl_skip=78.16% | acc_incl_skip=78.16%  ||  eval_2(back)=88개 acc_excl_skip=75.00% | acc_incl_skip=75.00%\n",
      "\n",
      "=== 전체 정확도 (파일 전체 합산) ===\n",
      "eval_1(front): correct=735, eval=998, total=998, skipped=0 -> acc_excl_skip=73.65% | acc_incl_skip=73.65%\n",
      "eval_2(back) : correct=741, eval=1002, total=1002, skipped=0 -> acc_excl_skip=73.95% | acc_incl_skip=73.95%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "IN_DIR = Path(\"./eval\")\n",
    "OUT1 = Path(\"./eval_1\")\n",
    "OUT2 = Path(\"./eval_2\")\n",
    "OUT1.mkdir(parents=True, exist_ok=True)\n",
    "OUT2.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def recalc_stats(data: dict, new_results: list) -> dict:\n",
    "    d = dict(data)\n",
    "    n_total = len(new_results)\n",
    "    n_skipped = sum(1 for r in new_results if r.get(\"is_skipped\", False))\n",
    "    n_eval = n_total - n_skipped\n",
    "    n_stopped = n_eval\n",
    "\n",
    "    n_correct = sum(\n",
    "        1 for r in new_results\n",
    "        if not r.get(\"is_skipped\", False) and r.get(\"is_correct\", False)\n",
    "    )\n",
    "    acc = f\"{(n_correct / n_eval) * 100:.2f}%\" if n_eval > 0 else \"0.00%\"\n",
    "\n",
    "    d[\"results\"] = new_results\n",
    "    d[\"num_items_total\"] = n_total\n",
    "    d[\"num_evaluated\"] = n_eval\n",
    "    d[\"num_stopped\"] = n_stopped\n",
    "    d[\"num_skipped\"] = n_skipped\n",
    "    d[\"accuracy_no_skipped\"] = acc\n",
    "    return d\n",
    "\n",
    "def count_metrics(results: list):\n",
    "    n_total = len(results)\n",
    "    n_skipped = sum(1 for r in results if r.get(\"is_skipped\", False))\n",
    "    n_eval = n_total - n_skipped\n",
    "    n_correct_excl_skip = sum(\n",
    "        1 for r in results\n",
    "        if not r.get(\"is_skipped\", False) and r.get(\"is_correct\", False)\n",
    "    )\n",
    "    acc_excl = (n_correct_excl_skip / n_eval * 100) if n_eval > 0 else 0.0\n",
    "    acc_incl = (n_correct_excl_skip / n_total * 100) if n_total > 0 else 0.0\n",
    "    return {\n",
    "        \"correct_excl\": n_correct_excl_skip,\n",
    "        \"eval_cnt\": n_eval,\n",
    "        \"total_cnt\": n_total,\n",
    "        \"skipped\": n_skipped,\n",
    "        \"acc_excl\": acc_excl,\n",
    "        \"acc_incl\": acc_incl,\n",
    "    }\n",
    "\n",
    "processed = []\n",
    "\n",
    "agg_1 = {\"correct_excl\": 0, \"eval_cnt\": 0, \"total_cnt\": 0, \"skipped\": 0}\n",
    "agg_2 = {\"correct_excl\": 0, \"eval_cnt\": 0, \"total_cnt\": 0, \"skipped\": 0}\n",
    "\n",
    "for path in sorted(IN_DIR.glob(\"*.json\")):\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    results = data.get(\"results\", [])\n",
    "    n = len(results)\n",
    "\n",
    "    n_front = n // 2\n",
    "    front = results[:n_front]\n",
    "    back = results[n_front:] \n",
    "\n",
    "    data_front = recalc_stats(data, front)\n",
    "    data_back = recalc_stats(data, back)\n",
    "\n",
    "    out1 = OUT1 / path.name\n",
    "    out2 = OUT2 / path.name\n",
    "\n",
    "    with out1.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data_front, f, ensure_ascii=False, indent=2)\n",
    "    with out2.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data_back, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    m1 = count_metrics(front)\n",
    "    m2 = count_metrics(back)\n",
    "\n",
    "    processed.append((\n",
    "        path.name,\n",
    "        len(front), len(back),\n",
    "        f\"{m1['acc_excl']:.2f}%\", f\"{m2['acc_excl']:.2f}%\",  \n",
    "        f\"{m1['acc_incl']:.2f}%\", f\"{m2['acc_incl']:.2f}%\", \n",
    "    ))\n",
    "\n",
    "    for k in agg_1: agg_1[k] += m1[k]\n",
    "    for k in agg_2: agg_2[k] += m2[k]\n",
    "\n",
    "for name, nf, nb, accf_ex, accb_ex, accf_in, accb_in in processed:\n",
    "    print(\n",
    "        f\"{name}: \"\n",
    "        f\"eval_1(front)={nf}개 acc_excl_skip={accf_ex} | acc_incl_skip={accf_in}  ||  \"\n",
    "        f\"eval_2(back)={nb}개 acc_excl_skip={accb_ex} | acc_incl_skip={accb_in}\"\n",
    "    )\n",
    "\n",
    "def pretty_overall(agg):\n",
    "    acc_excl = (agg[\"correct_excl\"] / agg[\"eval_cnt\"] * 100) if agg[\"eval_cnt\"] > 0 else 0.0\n",
    "    acc_incl = (agg[\"correct_excl\"] / agg[\"total_cnt\"] * 100) if agg[\"total_cnt\"] > 0 else 0.0\n",
    "    return acc_excl, acc_incl\n",
    "\n",
    "acc1_ex, acc1_in = pretty_overall(agg_1)\n",
    "acc2_ex, acc2_in = pretty_overall(agg_2)\n",
    "\n",
    "print(f\"eval_1(front): \"\n",
    "      f\"correct={agg_1['correct_excl']}, \"\n",
    "      f\"eval={agg_1['eval_cnt']}, total={agg_1['total_cnt']}, skipped={agg_1['skipped']} -> \"\n",
    "      f\"acc_excl_skip={acc1_ex:.2f}% | acc_incl_skip={acc1_in:.2f}%\")\n",
    "print(f\"eval_2(back) : \"\n",
    "      f\"correct={agg_2['correct_excl']}, \"\n",
    "      f\"eval={agg_2['eval_cnt']}, total={agg_2['total_cnt']}, skipped={agg_2['skipped']} -> \"\n",
    "      f\"acc_excl_skip={acc2_ex:.2f}% | acc_incl_skip={acc2_in:.2f}%\")\n",
    "\n",
    "def peek(dir_name: str, file_name: str):\n",
    "    p = Path(dir_name) / file_name\n",
    "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        d = json.load(f)\n",
    "    head = {k: d[k] for k in [\"model_id\", \"num_items_total\", \"num_evaluated\", \"num_stopped\", \"num_skipped\", \"accuracy_no_skipped\"] if k in d}\n",
    "    print(p, \"->\", head)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
