{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fc530f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nclabterm1/refresh/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, re, json, datetime, sys\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "RE_OPT = re.compile(r\"^[ \\t]*([A-Ea-e])[.)]\\s*(.+?)\\s*$\", re.MULTILINE)\n",
    "\n",
    "def parse_options(text: str) -> Tuple[List[str], Dict[str, str]]:\n",
    "    letters, mapping, seen = [], {}, set()\n",
    "    for m in RE_OPT.finditer(text or \"\"):\n",
    "        L = m.group(1).upper()\n",
    "        opt = m.group(2).strip()\n",
    "        if L not in seen:\n",
    "            seen.add(L)\n",
    "            letters.append(L)\n",
    "            mapping[L] = opt\n",
    "    return letters, mapping\n",
    "\n",
    "def bnb_config(dtype: torch.dtype):\n",
    "    return BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=dtype,\n",
    "        bnb_4bit_quant_storage=torch.uint8,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2948f424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "load_dotenv()\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "login(token)\n",
    "BASE_MODEL_ID = \"google/gemma-3-4b-it\"\n",
    "  \n",
    "SEGMENT = \"train\"                              \n",
    "USE_GENERATE = False                        \n",
    "MAX_ITEMS = None                           \n",
    "\n",
    "TORCH_DTYPE = torch.bfloat16\n",
    "\n",
    "USE_4BIT = True\n",
    "\n",
    "ATTN_IMPL = \"eager\"\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from train.build_dataset_v2 import concat_datasets \n",
    "\n",
    "USE_TEST_DATASETS_LIST = [\n",
    "    [\"../dataset/my_korean\", 900], \n",
    "    [\"../dataset/my_race_middle\", 3500], \n",
    "    [\"../dataset/my_race_high\", 10500], \n",
    "    [\"../dataset/my_cloth\", 5100],\n",
    "]\n",
    "\n",
    "raw0 = load_from_disk(USE_TEST_DATASETS_LIST[0][0])[\"train\"]\n",
    "raw1 = load_from_disk(USE_TEST_DATASETS_LIST[1][0])[\"train\"]\n",
    "raw2 = load_from_disk(USE_TEST_DATASETS_LIST[2][0])[\"train\"]\n",
    "raw3 = load_from_disk(USE_TEST_DATASETS_LIST[3][0])[\"train\"]\n",
    "\n",
    "dataset0 = raw0.shuffle(seed=42).select(range(USE_TEST_DATASETS_LIST[0][1]))\n",
    "dataset1 = raw1.shuffle(seed=42).select(range(USE_TEST_DATASETS_LIST[1][1]))\n",
    "dataset2 = raw2.shuffle(seed=42).select(range(USE_TEST_DATASETS_LIST[2][1]))\n",
    "dataset3 = raw3.shuffle(seed=42).select(range(USE_TEST_DATASETS_LIST[3][1]))\n",
    "\n",
    "dataset = concatenate_datasets([dataset0, dataset1, dataset2, dataset3])\n",
    "dataset = dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a89aeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "tok_id = BASE_MODEL_ID\n",
    "tokenizer = AutoTokenizer.from_pretrained(tok_id, use_fast=True)\n",
    "\n",
    "qconf = bnb_config(TORCH_DTYPE) if USE_4BIT else None\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    torch_dtype=TORCH_DTYPE,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=ATTN_IMPL,\n",
    "    quantization_config=qconf,\n",
    ")\n",
    "\n",
    "model = base\n",
    "device = next(model.parameters()).device\n",
    "model.eval()\n",
    "print(\"Loaded on:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca809d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 samples\n",
      "Dataset({\n",
      "    features: ['example_id', 'article', 'question', 'options', 'answer'],\n",
      "    num_rows: 20000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset), \"samples\")\n",
    "print(dataset) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd46bafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GENERATED ===\n",
      "1. Main: discuss food safety incidents\n",
      "2. Text: KFC's food safety issues\n",
      "3. A: introduce food safety incidents\n",
      "4. D: appeal to people for food safety\n",
      "answer is: D\n",
      "{'role': 'assistant', 'content': 'D'}\n"
     ]
    }
   ],
   "source": [
    "import train.build_dataset_v2 as bd\n",
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    item = dataset[119]\n",
    "    item = bd.create_conversation(item)\n",
    "    msgs = item[\"messages\"][:2]  \n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        msgs,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=False \n",
    "    )\n",
    "\n",
    "    gen_text = tokenizer.decode(\n",
    "        out[0, inputs.input_ids.shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "print(\"=== GENERATED ===\")\n",
    "print(gen_text)\n",
    "# print(item[\"messages\"][1])\n",
    "print(item[\"messages\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f658e814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_gen_start_for_row(inputs, out_ids, row_idx):\n",
    "    attn = inputs.attention_mask[row_idx].bool()\n",
    "    prompt_ids = inputs.input_ids[row_idx][attn]\n",
    "    seq = out_ids[row_idx]\n",
    "\n",
    "    gen_start = None\n",
    "    plen = prompt_ids.size(0)\n",
    "    limit = seq.size(0) - plen\n",
    "    for s in range(max(0, limit - 4), limit + 1):\n",
    "        if torch.equal(seq[s:s+plen], prompt_ids):\n",
    "            gen_start = s + plen\n",
    "            break\n",
    "    if gen_start is None:\n",
    "        for s in range(0, limit + 1):\n",
    "            if torch.equal(seq[s:s+plen], prompt_ids):\n",
    "                gen_start = s + plen\n",
    "        if gen_start is None:\n",
    "            gen_start = plen\n",
    "    return gen_start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afa4be6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba32d76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:   0%|          | 0/5000 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 5000/5000 [6:09:05<00:00,  4.43s/batch, acc=63.90%, att=2e+4, ok=12780, unpars=53, gold_miss=0, toks/s=55.7, reduced_batches=0, zero_budget=0]   \n"
     ]
    }
   ],
   "source": [
    "import re, time, copy, math, logging, torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(message)s\")\n",
    "\n",
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\"\n",
    "if getattr(tokenizer, \"pad_token_id\", None) is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "ANS_PATTERNS = [\n",
    "    re.compile(r\"answer\\s*is[:\\s]*([A-Z])\\b\", re.IGNORECASE),\n",
    "    re.compile(r\"\\b([A-E])\\b\") \n",
    "]\n",
    "\n",
    "def extract_letter_from_text(text: str) -> str | None:\n",
    "    m = ANS_PATTERNS[0].search(text or \"\")\n",
    "    if m: return m.group(1).upper()\n",
    "    cands = ANS_PATTERNS[1].findall(text or \"\")\n",
    "    return cands[-1].upper() if cands else None\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "MAX_NEW_TOKENS = 256  \n",
    "\n",
    "def generate_preds(dataset, batch_size, max_new_tokens):\n",
    "    n = len(dataset)\n",
    "    pred_list    = [None]  * n\n",
    "    correct_list = [False] * n\n",
    "    attempted = correct = unparsable = gold_missing = 0\n",
    "    gen_time_sum = 0.0\n",
    "    gen_tokens_sum = 0\n",
    "    reduced_budget_batches = 0\n",
    "    zero_budget_samples = 0\n",
    "\n",
    "    num_batches = math.ceil(n / batch_size)\n",
    "    bar = tqdm(range(num_batches), desc=\"Generating\", unit=\"batch\")\n",
    "\n",
    "    for b in bar:\n",
    "        start = b * batch_size\n",
    "        end   = min(start + batch_size, n)\n",
    "\n",
    "        batch_indices, batch_prompts = [], []\n",
    "        for idx in range(start, end):\n",
    "            item = dataset[idx]\n",
    "            item = bd.create_conversation(item)\n",
    "            if \"messages\" not in item or len(item[\"messages\"]) < 2:\n",
    "                continue\n",
    "            msgs = item[\"messages\"][:2]\n",
    "            prompt = tokenizer.apply_chat_template(\n",
    "                msgs, add_generation_prompt=True, tokenize=False\n",
    "            )\n",
    "            batch_indices.append(idx)\n",
    "            batch_prompts.append(prompt)\n",
    "\n",
    "        if not batch_prompts:\n",
    "            bar.set_postfix({\"acc\":\"n/a\",\"att\":attempted,\"ok\":correct,\n",
    "                            \"unpars\":unparsable,\"gold_miss\":gold_missing})\n",
    "            continue\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            batch_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=False \n",
    "        ).to(device)\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            out_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False\n",
    "            )\n",
    "        dt = time.perf_counter() - t0\n",
    "        gen_time_sum += dt\n",
    "\n",
    "        for bi, idx in enumerate(batch_indices):\n",
    "            gen_start = find_gen_start_for_row(inputs, out_ids, bi)\n",
    "            old_tokens = out_ids[bi, :gen_start]\n",
    "            new_tokens = out_ids[bi, gen_start:]\n",
    "            input_text = tokenizer.decode(old_tokens, skip_special_tokens=True)\n",
    "            gen_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "            gen_tokens_sum += int(new_tokens.numel())\n",
    "\n",
    "            gold = dataset[idx][\"answer\"]\n",
    "\n",
    "            # print(\"=====in=====\")\n",
    "            # print(input_text)\n",
    "            # print(\"=====gen=====\")\n",
    "            # print(gen_text)\n",
    "            # print(gold)\n",
    "            \n",
    "            if gold is None:\n",
    "                gold_missing += 1\n",
    "\n",
    "            pred = extract_letter_from_text(gen_text)\n",
    "            if pred is None:\n",
    "                unparsable += 1\n",
    "                attempted += 1\n",
    "                pred_list[idx] = None\n",
    "                correct_list[idx] = False\n",
    "            else:\n",
    "                attempted += 1\n",
    "                ok = (gold is not None and pred == gold)\n",
    "                correct += int(ok)\n",
    "                pred_list[idx] = gen_text\n",
    "                correct_list[idx] = ok\n",
    "\n",
    "        acc = (correct / attempted * 100) if attempted else 0.0\n",
    "        toks_per_s = (gen_tokens_sum / gen_time_sum) if gen_time_sum > 0 else 0.0\n",
    "        bar.set_postfix({\n",
    "            \"acc\": f\"{acc:.2f}%\",\n",
    "            \"att\": attempted,\n",
    "            \"ok\": correct,\n",
    "            \"unpars\": unparsable,\n",
    "            \"gold_miss\": gold_missing,\n",
    "            \"toks/s\": f\"{toks_per_s:.1f}\",\n",
    "            \"reduced_batches\": reduced_budget_batches,\n",
    "            \"zero_budget\": zero_budget_samples\n",
    "        })\n",
    "\n",
    "    dataset_copy = dataset.add_column(\"pred\", pred_list)\n",
    "    dataset_copy = dataset_copy.add_column(\"correct\", correct_list)\n",
    "    return dataset_copy\n",
    "\n",
    "result = generate_preds(dataset, BATCH_SIZE, MAX_NEW_TOKENS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64138418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 20000/20000 [00:00<00:00, 598758.60 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved HF Arrow shards to: ./generate_output/init_generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os, datetime\n",
    "\n",
    "OUT_DIR = \"./generate_output\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "base = os.path.join(OUT_DIR, \"init_generate\")\n",
    "\n",
    "result.save_to_disk(base) \n",
    "print(f\"Saved HF Arrow shards to: {base}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89b0978",
   "metadata": {},
   "source": [
    "Rationalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dcd8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 20000 rows; columns: ['example_id', 'article', 'question', 'options', 'answer', 'pred', 'correct']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 20000/20000 [00:00<00:00, 99553.87 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect: 7220\n",
      "---\n",
      "pred: 1. Context: power outage\n",
      "2. Text: something blocked power\n",
      "3. A: poles (electricity poles)\n",
      "answer is: A\n",
      "correct flag: False\n",
      "---\n",
      "pred: 1. Context: growth and becoming more real\n",
      "2. Best fit: A. cleverer (implies growth in understanding and maturity)\n",
      "correct flag: False\n",
      "---\n",
      "pred: 1. Identify the main point about Descartes' experience.\n",
      "2. Recognize the role of dreams in Descartes' discovery.\n",
      "3. Understand that the discovery was significant and influenced his life path.\n",
      "4. Realize that the dream contributed to an important discovery, not always the case.\n",
      "answer is: B. Dreams sometimes contribute to important discoveries.\n",
      "correct flag: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import train.build_dataset_rationalization as bdr\n",
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk(\"./generate_output/init_generate\")\n",
    "print(\"Loaded:\", ds.num_rows, \"rows; columns:\", ds.column_names)\n",
    "\n",
    "def is_incorrect(ex):\n",
    "    v = ex[\"correct\"] if \"correct\" in ex else False\n",
    "    if isinstance(v, bool):\n",
    "        return (v is False)\n",
    "    if isinstance(v, str):\n",
    "        return v.strip().lower() in {\"false\", \"0\", \"no\"}\n",
    "    if isinstance(v, int):\n",
    "        return v == 0\n",
    "    return True\n",
    "\n",
    "incorrect = ds.filter(is_incorrect)\n",
    "print(\"Incorrect:\", incorrect.num_rows)\n",
    "\n",
    "for i in range(min(3, incorrect.num_rows)):\n",
    "    print(\"---\")\n",
    "    print(\"pred:\", incorrect[i].get(\"pred\"))\n",
    "    print(\"correct flag:\", incorrect[i].get(\"correct\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f8ee35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_preds_rationalization(dataset, batch_size, max_new_tokens):\n",
    "    n = len(dataset)\n",
    "    pred_list    = [None]  * n\n",
    "    correct_list = [False] * n\n",
    "    attempted = correct = unparsable = gold_missing = 0\n",
    "    gen_time_sum = 0.0\n",
    "    gen_tokens_sum = 0\n",
    "    reduced_budget_batches = 0\n",
    "    zero_budget_samples = 0\n",
    "\n",
    "    num_batches = math.ceil(n / batch_size)\n",
    "    bar = tqdm(range(num_batches), desc=\"Generating\", unit=\"batch\")\n",
    "\n",
    "    for b in bar:\n",
    "        start = b * batch_size\n",
    "        end   = min(start + batch_size, n)\n",
    "\n",
    "        batch_indices, batch_prompts = [], []\n",
    "        for idx in range(start, end):\n",
    "            item = dataset[idx]\n",
    "            item = bd.create_conversation_for_rational(item)\n",
    "            if \"messages\" not in item or len(item[\"messages\"]) < 2:\n",
    "                continue\n",
    "            msgs = item[\"messages\"][:2]\n",
    "            prompt = tokenizer.apply_chat_template(\n",
    "                msgs, add_generation_prompt=True, tokenize=False\n",
    "            )\n",
    "            batch_indices.append(idx)\n",
    "            batch_prompts.append(prompt)\n",
    "\n",
    "        if not batch_prompts:\n",
    "            bar.set_postfix({\"acc\":\"n/a\",\"att\":attempted,\"ok\":correct,\n",
    "                            \"unpars\":unparsable,\"gold_miss\":gold_missing})\n",
    "            continue\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            batch_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=False \n",
    "        ).to(device)\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            out_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False\n",
    "            )\n",
    "        dt = time.perf_counter() - t0\n",
    "        gen_time_sum += dt\n",
    "\n",
    "        for bi, idx in enumerate(batch_indices):\n",
    "            gen_start = find_gen_start_for_row(inputs, out_ids, bi)\n",
    "            old_tokens = out_ids[bi, :gen_start]\n",
    "            new_tokens = out_ids[bi, gen_start:]\n",
    "            input_text = tokenizer.decode(old_tokens, skip_special_tokens=True)\n",
    "            gen_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "            gen_tokens_sum += int(new_tokens.numel())\n",
    "\n",
    "            gold = dataset[idx][\"answer\"]\n",
    "\n",
    "            # print(\"=====in=====\")\n",
    "            # print(input_text)\n",
    "            # print(\"=====gen=====\")\n",
    "            # print(gen_text)\n",
    "            # print(gold)\n",
    "            \n",
    "            if gold is None:\n",
    "                gold_missing += 1\n",
    "\n",
    "            pred = extract_letter_from_text(gen_text)\n",
    "            if pred is None:\n",
    "                unparsable += 1\n",
    "                attempted += 1\n",
    "                pred_list[idx] = None\n",
    "                correct_list[idx] = False\n",
    "            else:\n",
    "                attempted += 1\n",
    "                ok = (gold is not None and pred == gold)\n",
    "                correct += int(ok)\n",
    "                pred_list[idx] = gen_text\n",
    "                correct_list[idx] = ok\n",
    "\n",
    "        acc = (correct / attempted * 100) if attempted else 0.0\n",
    "        toks_per_s = (gen_tokens_sum / gen_time_sum) if gen_time_sum > 0 else 0.0\n",
    "        bar.set_postfix({\n",
    "            \"acc\": f\"{acc:.2f}%\",\n",
    "            \"att\": attempted,\n",
    "            \"ok\": correct,\n",
    "            \"unpars\": unparsable,\n",
    "            \"gold_miss\": gold_missing,\n",
    "            \"toks/s\": f\"{toks_per_s:.1f}\",\n",
    "            \"reduced_batches\": reduced_budget_batches,\n",
    "            \"zero_budget\": zero_budget_samples\n",
    "        })\n",
    "        \n",
    "    dataset_copy = dataset.remove_columns(\"pred\")\n",
    "    dataset_copy = dataset_copy.remove_columns(\"correct\")\n",
    "    dataset_copy = dataset_copy.add_column(\"pred\", pred_list)\n",
    "    dataset_copy = dataset_copy.add_column(\"correct\", correct_list)\n",
    "    return dataset_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "923c9330",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 1805/1805 [3:27:10<00:00,  6.89s/batch, acc=95.76%, att=7220, ok=6914, unpars=90, gold_miss=0, toks/s=61.1, reduced_batches=0, zero_budget=0]  \n",
      "Flattening the indices: 100%|██████████| 7220/7220 [00:00<00:00, 35265.30 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 7220/7220 [00:00<00:00, 578889.64 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved HF Arrow shards to: ./generate_output/init_rationalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rationalization = generate_preds_rationalization(incorrect, BATCH_SIZE, MAX_NEW_TOKENS)\n",
    "\n",
    "base2 = os.path.join(OUT_DIR, \"init_rationalization\")\n",
    "rationalization.save_to_disk(base2) \n",
    "print(f\"Saved HF Arrow shards to: {base2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a7f1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 20000/20000 [00:00<00:00, 1164405.21 examples/s]\n",
      "Filter:   0%|          | 0/7220 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 7220/7220 [00:00<00:00, 950438.61 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets, Dataset\n",
    "ds1 = load_from_disk(\"./generate_output/init_generate\")\n",
    "ds2 = load_from_disk(\"./generate_output/init_rationalization\")\n",
    "\n",
    "def drop_incorrect(ds):\n",
    "    def _filter(dset):\n",
    "        return dset.filter(\n",
    "            lambda batch: [bool(x) for x in batch[\"correct\"]],\n",
    "            batched=True,\n",
    "        )\n",
    "    if isinstance(ds, Dataset):\n",
    "        return _filter(ds)\n",
    "    else:\n",
    "        raise TypeError(\"Unknown dataset type\")\n",
    "\n",
    "ds1_ok = drop_incorrect(ds1)\n",
    "ds2_ok = drop_incorrect(ds2)\n",
    "\n",
    "merge = concatenate_datasets([ds1_ok, ds2_ok])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cec7b47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Sonora Smart Dodd's father, Henry Jackson Smart, inspired the idea for Father's Day.\n",
      "2. Henry Jackson Smart was kind to his daughter, Sonora Smart Dodd.\n",
      "3. Options A, C, D are not directly related to the passage.\n",
      "answer is: B. did a lot for his daughter\n",
      "B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 19694/19694 [00:00<00:00, 141959.40 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved HF Arrow shards to: ./generate_output/init_merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "a = 2\n",
    "print(merge[a]['pred'])\n",
    "print(merge[a]['answer'])\n",
    "base3 = os.path.join(OUT_DIR, \"init_merge\")\n",
    "merge.save_to_disk(base3) \n",
    "print(f\"Saved HF Arrow shards to: {base3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3da0425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting to class labels: 100%|██████████| 19694/19694 [00:00<00:00, 340120.24 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['example_id', 'article', 'question', 'options', 'answer', 'pred', 'correct'],\n",
      "        num_rows: 15755\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['example_id', 'article', 'question', 'options', 'answer', 'pred', 'correct'],\n",
      "        num_rows: 1969\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['example_id', 'article', 'question', 'options', 'answer', 'pred', 'correct'],\n",
      "        num_rows: 1970\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 15755/15755 [00:00<00:00, 146711.62 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1969/1969 [00:00<00:00, 126682.89 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1970/1970 [00:00<00:00, 131822.70 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: ./generate_output/loop0_dataset\n",
      "sizes: {'train': 15755, 'validation': 1969, 'test': 1970}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk, DatasetDict\n",
    "from datasets import ClassLabel\n",
    "\n",
    "DATA_DIR_IN  = \"./generate_output/init_merge\"\n",
    "DATA_DIR_OUT = \"./generate_output/loop0_dataset\"\n",
    "SEED = 42\n",
    "\n",
    "ds = load_from_disk(DATA_DIR_IN)\n",
    "\n",
    "unique_answers = sorted(set(ds[\"answer\"]))\n",
    "class_label = ClassLabel(num_classes=len(unique_answers), names=unique_answers)\n",
    "\n",
    "ds_encoded = ds.class_encode_column(\"answer\")\n",
    "\n",
    "split1 = ds_encoded.train_test_split(test_size=0.2, seed=SEED, stratify_by_column=\"answer\")\n",
    "\n",
    "train = split1[\"train\"]\n",
    "temp  = split1[\"test\"]\n",
    "\n",
    "split2 = temp.train_test_split(test_size=0.5, seed=SEED, stratify_by_column=\"answer\")\n",
    "\n",
    "validation = split2[\"train\"]\n",
    "test       = split2[\"test\"]\n",
    "\n",
    "dd = DatasetDict({\n",
    "    \"train\": train,\n",
    "    \"validation\": validation,\n",
    "    \"test\": test,\n",
    "})\n",
    "\n",
    "print(dd)\n",
    "dd.save_to_disk(DATA_DIR_OUT)\n",
    "print(f\"Saved to: {DATA_DIR_OUT}\")\n",
    "print(\"sizes:\", {k: v.num_rows for k, v in dd.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1870909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk(\"./generate_output/loop0_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49400bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15755/15755 [00:00<00:00, 368157.13 examples/s]\n",
      "Map: 100%|██████████| 1969/1969 [00:00<00:00, 274947.05 examples/s]\n",
      "Map: 100%|██████████| 1970/1970 [00:00<00:00, 277079.20 examples/s]\n"
     ]
    }
   ],
   "source": [
    "mapping = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\", 4: \"E\"}\n",
    "\n",
    "def to_letter(batch):\n",
    "    out = []\n",
    "    for v in batch[\"answer\"]:\n",
    "        if isinstance(v, int):\n",
    "            out.append(mapping.get(v, str(v)))\n",
    "        elif isinstance(v, str):\n",
    "            if v.isdigit():\n",
    "                out.append(mapping.get(int(v), v))\n",
    "            else:\n",
    "                out.append(v)\n",
    "        else:\n",
    "            out.append(str(v))\n",
    "    return {\"answer_letter\": out}\n",
    "\n",
    "train2 = ds[\"train\"].map(to_letter, batched=True, load_from_cache_file=False)\n",
    "train2 = train2.remove_columns(\"answer\").rename_column(\"answer_letter\", \"answer\")\n",
    "\n",
    "vali2 = ds[\"validation\"].map(to_letter, batched=True, load_from_cache_file=False)\n",
    "vali2 = vali2.remove_columns(\"answer\").rename_column(\"answer_letter\", \"answer\")\n",
    "\n",
    "test2 = ds[\"test\"].map(to_letter, batched=True, load_from_cache_file=False)\n",
    "test2 = test2.remove_columns(\"answer\").rename_column(\"answer_letter\", \"answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3170b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "complete = DatasetDict({\n",
    "    \"train\": train2,\n",
    "    \"validation\": vali2,\n",
    "    \"test\": test2,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae9f2f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 15755/15755 [00:00<00:00, 595160.45 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1969/1969 [00:00<00:00, 340433.84 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1970/1970 [00:00<00:00, 359924.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "complete.save_to_disk(\"./dataset/loop0_dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
